{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a49ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings, OpenAIEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from urllib.request import urlretrieve\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79ab1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(r\"C:\\Users\\mahdi\\OneDrive\\Desktop\\chatbotdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "445202e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_before_split = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df80a0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft速 PowerPoint速 2016', 'creator': 'Microsoft速 PowerPoint速 2016', 'creationdate': '2022-02-27T10:48:35+02:00', 'title': 'Electromagnetic Waves & Acoustics', 'author': 'Hamza ISSA', 'moddate': '2022-02-27T10:48:35+02:00', 'source': 'C:\\\\Users\\\\mahdi\\\\OneDrive\\\\Desktop\\\\chatbotdata\\\\Chapter 3-Lecture1 .pdf', 'total_pages': 25, 'page': 1, 'page_label': '2'}, page_content='I. Plan\\nReflection and Transmission Coefficient Measurement\\nII. Introduction\\nIII. Directional Coupler\\nIV. Measurement of Ref. and Tx Coefficients\\nA. O/P Power Control of signal Generators\\nB. Measurement of Reflection Coefficient using Slotted Line measurement \\ntechnique\\nC. Scalar Analyzer\\nD. Vector Network Analyzer\\nD.1. Complex Impedance Bridge\\nD.2. Transmission Coefficient Bridge\\nD.3. Heterodyne VNA.\\nD.4. Six-Port technique\\ni. Six-Port Reflectometer. ii. Six-Port Network Analyzer. \\nChapter 3\\n2')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_before_split[1\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdb4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter =  RecursiveCharacterTextSplitter(\n",
    "    chunk_size =700,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25b4e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_after_split[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ee7770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8a69d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6126ee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before split: 326\n",
      "after split: 307\n"
     ]
    }
   ],
   "source": [
    "print(f'before split: {avg_char_before_split}')\n",
    "print(f'after split: {avg_char_after_split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84453b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name= \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs = {'device' : 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings' : True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "441569ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.034233853220939636,\n",
       " 0.14126251637935638,\n",
       " -0.020818151533603668,\n",
       " -0.010789905674755573,\n",
       " -0.06833900511264801,\n",
       " 0.07193399220705032,\n",
       " 0.09262653440237045,\n",
       " -0.045325133949518204,\n",
       " 0.006242727395147085,\n",
       " 0.008324368856847286,\n",
       " -0.010593184269964695,\n",
       " -0.01494328398257494,\n",
       " -0.0037628139834851027,\n",
       " -0.018284481018781662,\n",
       " 0.010794136673212051,\n",
       " -0.03792990744113922,\n",
       " -0.08044886589050293,\n",
       " -0.012282311916351318,\n",
       " 0.030246837064623833,\n",
       " -0.03435514494776726,\n",
       " 0.12390776723623276,\n",
       " 0.09385240823030472,\n",
       " -0.05612576752901077,\n",
       " -0.07806669175624847,\n",
       " -0.07746580988168716,\n",
       " 0.014936726540327072,\n",
       " -0.017674215137958527,\n",
       " -0.0028217239305377007,\n",
       " 0.08591408282518387,\n",
       " -0.07451906055212021,\n",
       " -0.001217714510858059,\n",
       " 0.037950821220874786,\n",
       " 0.07088372111320496,\n",
       " -0.0016862088814377785,\n",
       " 0.03360874950885773,\n",
       " 0.06980914622545242,\n",
       " 0.059654634445905685,\n",
       " 0.006904518697410822,\n",
       " 0.034787148237228394,\n",
       " -0.04653768241405487,\n",
       " -0.005491605028510094,\n",
       " -0.04319872707128525,\n",
       " -0.0020654588006436825,\n",
       " 0.028355592861771584,\n",
       " 0.029407989233732224,\n",
       " 0.0327128991484642,\n",
       " -0.026088057085871696,\n",
       " 0.039575252681970596,\n",
       " -0.005508308298885822,\n",
       " -0.06366078555583954,\n",
       " -0.07902470231056213,\n",
       " -0.06549139320850372,\n",
       " -0.06111622974276543,\n",
       " 0.014392801560461521,\n",
       " 0.04167933017015457,\n",
       " -0.02443758398294449,\n",
       " -0.046191781759262085,\n",
       " -0.003639098722487688,\n",
       " 0.014619477093219757,\n",
       " -0.06558084487915039,\n",
       " 0.02734319493174553,\n",
       " 0.017643388360738754,\n",
       " 0.006735681090503931,\n",
       " 0.010108468122780323,\n",
       " 0.03992777690291405,\n",
       " -0.05074656754732132,\n",
       " 0.07384271174669266,\n",
       " -0.026806898415088654,\n",
       " -0.09092092514038086,\n",
       " 0.049835942685604095,\n",
       " 0.07237226516008377,\n",
       " -0.01793723925948143,\n",
       " -0.08263734728097916,\n",
       " -0.0384899266064167,\n",
       " -0.10488124191761017,\n",
       " -0.031142745167016983,\n",
       " -0.06066224351525307,\n",
       " -0.055062804371118546,\n",
       " 0.009326768107712269,\n",
       " -0.05774947255849838,\n",
       " 0.011084210127592087,\n",
       " 0.03867969289422035,\n",
       " 0.02817409299314022,\n",
       " 0.08288534730672836,\n",
       " -0.01975434087216854,\n",
       " 0.03728358447551727,\n",
       " -0.011042019352316856,\n",
       " -0.12034844607114792,\n",
       " 0.021085523068904877,\n",
       " 0.017684267833828926,\n",
       " -0.04565493389964104,\n",
       " -0.14971183240413666,\n",
       " 0.022306004539132118,\n",
       " 0.008606191724538803,\n",
       " 0.02544986456632614,\n",
       " 0.02964576706290245,\n",
       " -0.007466061040759087,\n",
       " 0.07633977383375168,\n",
       " 0.018687745556235313,\n",
       " 0.03788474574685097,\n",
       " 0.08060706406831741,\n",
       " 0.031002717092633247,\n",
       " -0.02340996451675892,\n",
       " -0.04598287120461464,\n",
       " -0.043336138129234314,\n",
       " 0.006617678329348564,\n",
       " -0.024776872247457504,\n",
       " 0.04097845032811165,\n",
       " 0.06973151117563248,\n",
       " -0.010094930417835712,\n",
       " -0.12494298070669174,\n",
       " -0.018543438985943794,\n",
       " -0.04070885851979256,\n",
       " -0.07440976053476334,\n",
       " 0.0285408403724432,\n",
       " -0.0393674373626709,\n",
       " 0.03452859818935394,\n",
       " 0.014358101412653923,\n",
       " -0.04111883044242859,\n",
       " 0.017392516136169434,\n",
       " -0.057546216994524,\n",
       " 0.04636608436703682,\n",
       " -0.027848346158862114,\n",
       " 0.032386425882577896,\n",
       " 0.023935750126838684,\n",
       " -0.04688669741153717,\n",
       " -0.08256050944328308,\n",
       " -1.8443735953142086e-33,\n",
       " -0.024089941754937172,\n",
       " 0.00921219028532505,\n",
       " -0.007850989699363708,\n",
       " 0.037579312920570374,\n",
       " -0.08958803117275238,\n",
       " -0.019080067053437233,\n",
       " -0.02522667869925499,\n",
       " -0.013410503044724464,\n",
       " 0.04589413106441498,\n",
       " -0.07515493780374527,\n",
       " -0.023906411603093147,\n",
       " 0.004124297294765711,\n",
       " -0.01714739389717579,\n",
       " -0.01126287505030632,\n",
       " 0.011753189377486706,\n",
       " -0.062433477491140366,\n",
       " -0.01800043135881424,\n",
       " -0.014240150339901447,\n",
       " -0.005667306017130613,\n",
       " 0.02836228907108307,\n",
       " 0.016594408079981804,\n",
       " 0.062031280249357224,\n",
       " -0.04528022184967995,\n",
       " -0.08617710322141647,\n",
       " 0.03508691489696503,\n",
       " -0.019241897389292717,\n",
       " 0.048186756670475006,\n",
       " -0.03848614916205406,\n",
       " 0.056862972676754,\n",
       " 0.017826322466135025,\n",
       " 0.019455034285783768,\n",
       " 0.009021471254527569,\n",
       " -0.015265273861587048,\n",
       " -0.01104375533759594,\n",
       " 0.08956926316022873,\n",
       " 0.0435105636715889,\n",
       " -0.007290378212928772,\n",
       " -0.026640955358743668,\n",
       " -0.06263246387243271,\n",
       " 0.012085628695786,\n",
       " 0.0745103731751442,\n",
       " 0.023680860176682472,\n",
       " -0.025191104039549828,\n",
       " -0.001902362098917365,\n",
       " 0.0834166407585144,\n",
       " 0.07307951897382736,\n",
       " -0.03374795988202095,\n",
       " 0.055465806275606155,\n",
       " 0.016885777935385704,\n",
       " -0.0013680988922715187,\n",
       " -0.005961221642792225,\n",
       " 0.05187578126788139,\n",
       " -0.017361553385853767,\n",
       " -0.037967607378959656,\n",
       " 0.05024225637316704,\n",
       " 0.06990996748209,\n",
       " -0.11479230225086212,\n",
       " 0.10484279692173004,\n",
       " -0.013778728432953358,\n",
       " 0.05630869045853615,\n",
       " -0.011107651516795158,\n",
       " -0.04897068068385124,\n",
       " 0.057562172412872314,\n",
       " 0.10075736045837402,\n",
       " -0.04139044135808945,\n",
       " -0.012359052896499634,\n",
       " -0.0013857934391126037,\n",
       " -6.399212725227699e-05,\n",
       " 0.027337733656167984,\n",
       " 0.0477793850004673,\n",
       " -0.033933915197849274,\n",
       " 0.002297112485393882,\n",
       " 0.011592032387852669,\n",
       " 0.132253959774971,\n",
       " -0.0018674780149012804,\n",
       " 0.01163685042411089,\n",
       " 0.00029018172062933445,\n",
       " 0.037967320531606674,\n",
       " -0.0290386825799942,\n",
       " 0.006413550581783056,\n",
       " -0.05547817051410675,\n",
       " 0.009403916075825691,\n",
       " -0.017381075769662857,\n",
       " 0.00908223632723093,\n",
       " 0.0004806078504770994,\n",
       " -0.0885094702243805,\n",
       " 0.06455030292272568,\n",
       " -0.14452588558197021,\n",
       " -0.0049819862470030785,\n",
       " 0.021754706278443336,\n",
       " 0.029266012832522392,\n",
       " 0.011870340444147587,\n",
       " -0.018007917329669,\n",
       " -0.01943395286798477,\n",
       " -0.053297657519578934,\n",
       " -9.34632803273916e-34,\n",
       " -0.05363130569458008,\n",
       " -0.025734014809131622,\n",
       " -0.04722873494029045,\n",
       " 0.0589948333799839,\n",
       " -0.05194137245416641,\n",
       " 0.00651811296120286,\n",
       " 0.029001517221331596,\n",
       " 0.03803277015686035,\n",
       " -0.006022091489285231,\n",
       " 0.015312152914702892,\n",
       " -0.05899842828512192,\n",
       " -0.05474068969488144,\n",
       " -0.00055466074263677,\n",
       " -0.03178628906607628,\n",
       " -0.08154971152544022,\n",
       " 0.005541279446333647,\n",
       " 0.0676727294921875,\n",
       " -0.02874712459743023,\n",
       " -0.010584240779280663,\n",
       " 0.056936878710985184,\n",
       " -0.06748566031455994,\n",
       " 0.08976395428180695,\n",
       " -0.0827898383140564,\n",
       " -0.07709282636642456,\n",
       " 0.029436452314257622,\n",
       " 0.09515032917261124,\n",
       " 0.01736142300069332,\n",
       " -0.02013360522687435,\n",
       " -0.022432036697864532,\n",
       " 0.03459459915757179,\n",
       " 0.09659743309020996,\n",
       " -0.029167111963033676,\n",
       " -0.13245640695095062,\n",
       " 0.013921312056481838,\n",
       " -0.031155243515968323,\n",
       " -0.012152263894677162,\n",
       " 0.08273231983184814,\n",
       " 0.049832675606012344,\n",
       " 0.07400395721197128,\n",
       " 0.01988912560045719,\n",
       " 1.3434926586342044e-05,\n",
       " 0.06570091098546982,\n",
       " 0.022054022178053856,\n",
       " 0.09586940705776215,\n",
       " 0.02501397207379341,\n",
       " 0.01612667739391327,\n",
       " -0.012590884231030941,\n",
       " 0.12368635833263397,\n",
       " 0.046442579478025436,\n",
       " -0.03926670178771019,\n",
       " -0.09209612011909485,\n",
       " -0.04529745876789093,\n",
       " -0.013833253644406796,\n",
       " -0.10814117640256882,\n",
       " 0.0595531202852726,\n",
       " -0.013441079296171665,\n",
       " -0.023974068462848663,\n",
       " -0.06780210137367249,\n",
       " -0.045143213123083115,\n",
       " 0.03953186050057411,\n",
       " -0.004440524615347385,\n",
       " 0.02424149215221405,\n",
       " -0.04387129843235016,\n",
       " 0.07729726284742355,\n",
       " -0.021711645647883415,\n",
       " -0.001768064801581204,\n",
       " -0.06478947401046753,\n",
       " -0.03161971643567085,\n",
       " 0.015362725593149662,\n",
       " -0.02266709692776203,\n",
       " -0.007922129705548286,\n",
       " -0.1689135581254959,\n",
       " -0.05812507122755051,\n",
       " 0.032440029084682465,\n",
       " -0.03270195424556732,\n",
       " 0.01581825129687786,\n",
       " 0.07494274526834488,\n",
       " -0.0369301401078701,\n",
       " -0.04045708850026131,\n",
       " -0.06917935609817505,\n",
       " 0.011706644669175148,\n",
       " -0.04811728373169899,\n",
       " -0.07797426730394363,\n",
       " 0.064154252409935,\n",
       " -0.02818458341062069,\n",
       " -0.026981886476278305,\n",
       " 0.029550712555646896,\n",
       " 0.005808951333165169,\n",
       " 0.02168283797800541,\n",
       " 0.04820316657423973,\n",
       " -0.05138584226369858,\n",
       " -0.03889761120080948,\n",
       " -0.0823211818933487,\n",
       " -0.0013201008550822735,\n",
       " 0.008392025716602802,\n",
       " -2.242668628582578e-08,\n",
       " -0.005140651948750019,\n",
       " -0.01211332157254219,\n",
       " 0.036055587232112885,\n",
       " -0.04279031231999397,\n",
       " 0.04000576585531235,\n",
       " -0.0028097510803490877,\n",
       " 0.08204251527786255,\n",
       " -0.0030142932664602995,\n",
       " 0.005598538555204868,\n",
       " 0.047789525240659714,\n",
       " 0.027402447536587715,\n",
       " 0.012669573538005352,\n",
       " -0.051680248230695724,\n",
       " -0.0517648346722126,\n",
       " 0.05132950469851494,\n",
       " 0.008720947429537773,\n",
       " 0.021465258672833443,\n",
       " -0.045009274035692215,\n",
       " -0.07011514902114868,\n",
       " -0.06582584232091904,\n",
       " 0.029499514028429985,\n",
       " 0.0036770831793546677,\n",
       " -0.053967952728271484,\n",
       " -0.04039203003048897,\n",
       " -0.0639050155878067,\n",
       " 0.08222229778766632,\n",
       " 0.08714134991168976,\n",
       " -0.014243442565202713,\n",
       " -0.05194036662578583,\n",
       " -0.0007619952666573226,\n",
       " 0.02838115580379963,\n",
       " 0.12976033985614777,\n",
       " -0.06375992298126221,\n",
       " 0.012545766308903694,\n",
       " 0.0021255933679640293,\n",
       " 0.03743089735507965,\n",
       " 0.02211536467075348,\n",
       " 0.07115688920021057,\n",
       " 0.104253388941288,\n",
       " -0.0345296673476696,\n",
       " 0.028036853298544884,\n",
       " 0.04651014506816864,\n",
       " 0.07857854664325714,\n",
       " 0.07019484788179398,\n",
       " 0.03999744728207588,\n",
       " -0.004220468457788229,\n",
       " 0.00823448970913887,\n",
       " 0.10111337155103683,\n",
       " -0.031731560826301575,\n",
       " -0.041523512452840805,\n",
       " -0.039177052676677704,\n",
       " 0.035679712891578674,\n",
       " 0.06668219715356827,\n",
       " -0.012633315287530422,\n",
       " -0.0007695478852838278,\n",
       " 0.027737541124224663,\n",
       " 0.005970746278762817,\n",
       " 0.03761359304189682,\n",
       " -0.048685088753700256,\n",
       " -0.07192370295524597,\n",
       " 0.11339909583330154,\n",
       " -0.02685646153986454,\n",
       " 0.05223971977829933,\n",
       " -0.03598601371049881]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_embeddings.embed_query(\"mename is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2a5d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "785e86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\" , search_kwargs={\"k\" : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f57405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token=\"hf_...NjxL\"\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc48d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "498615d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_0 = [{\"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\"}]\n",
    "response =client.chat.completions.create(\n",
    "    model='phi:latest',\n",
    "    temperature=0.3,\n",
    "    messages = messages_0\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5f5670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34976729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"phi:latest\",\n",
    "    openai_api_base=\"http://localhost:11434/v1\",\n",
    "    openai_api_key=\"nokeyneeded\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "415f6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs = {\"prompt\" : PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25fcc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_with_model(history, new_message):\n",
    "    \n",
    "    result = retrievalQA.invoke({\"query\": new_message})\n",
    "    \n",
    "    # Extract the answer from the result\n",
    "    assistant_message = result.get(\"result\") or result.get(\"answer\") or str(result)\n",
    "    \n",
    "   \n",
    "    history.append((new_message, assistant_message))\n",
    "    \n",
    "    \n",
    "    return history, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09acc20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "665bf9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_2180\\1986725830.py:7: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label = \"Chat Interface\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\gradio\\blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_2180\\2637622776.py\", line 3, in chat_with_model\n",
      "    result = retrievalQA.invoke({\"query\": new_message})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py\", line 154, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 608, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 319, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 766, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 971, in generate\n",
      "    return self._generate_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 792, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py\", line 463, in _generate\n",
      "    response = completion_with_retry(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py\", line 121, in completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\openai\\resources\\completions.py\", line 541, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1256, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mahdi\\OneDrive\\Desktop\\lara wehbi\\aivenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field CompletionRequest.prompt of type string', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr \n",
    "def gradio_chat_app():\n",
    "    with gr.Blocks() as app:\n",
    "        gr.Markdown(\"# Ollam Phi Model Chat Interface\")\n",
    "        gr.Markdown(\"Chat with the Phi model in a conversational format.\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(label = \"Chat Interface\")\n",
    "        user_input = gr.Textbox(label = \"your message\" , placeholder = \"Type something ...\" , lines=1)  \n",
    "        send_button = gr.Button(\"send\")\n",
    "        \n",
    "        def clear_chat():\n",
    "            return [] , \"\"\n",
    "    \n",
    "        clear_button = gr.Button(\"Clear chat\")\n",
    "        \n",
    "        send_button.click(\n",
    "            fn=chat_with_model, \n",
    "            inputs = [chatbot, user_input],\n",
    "            outputs = [chatbot, user_input]\n",
    "        )\n",
    "        clear_button.click(\n",
    "            fn=clear_chat,\n",
    "            inputs = [],\n",
    "            outputs = [chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "    return app\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = gradio_chat_app()\n",
    "    app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
